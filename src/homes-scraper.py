import requests
import csv
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from random import shuffle

last_zip = ''
last_page = ''

class CityScraper:

	base_url = 'https://www.homes.com/home-prices/'

	def __init__(self, state, city, url, file_loc, debug=False):
		self.state = state
		self.city = city
		self.url = url
		#self.df = pd.Dataframe(columns=['long', 'lat', 'addr', 'city', 'state', 'zip', 'url', 'beds', 'baths', 'sqft', 'built', 'price'])
		self.debug = debug
		self.out_file = open(file_loc, 'wt', newline='')
		self.writer = csv.writer(self.out_file)
		self.writer.writerow(('long', 'lat', 'addr', 'city', 'state', 'zip', 'url', 'beds', 'baths', 'sqft', 'date', 'price'))
		self.beds_threshold = 7
		self.baths_threshold = 7
		self.sqft_threshold = 300
		self.built_threshold = 1899
		self.price_threshold = 30000
		self.driver = webdriver.Firefox()
		self.driver.set_page_load_timeout(5)
		if debug:
			self.log_file = open('log.txt', 'w')
			self.fails = 0
			self.fail_urls = []
			self.datapoints = 0
			print('CityScraper finished init')

	# Returns whether the information is acceptable
	def data_acceptable(self, beds, baths, sqft, built, price):
		if not (len(beds) > 0 and len(baths) > 0 and len(sqft) > 0 and len(built) > 0 and len(price) > 0):
			return False
		if not (beds.isdigit() and baths.isdigit() and sqft.isdigit() and built.isdigit() and price.isdigit()):
			return False
		if int(beds) > self.beds_threshold or int(beds) < 1:
			return False
		if int(baths) > self.baths_threshold or int(baths) < 1:
			return False
		if int(sqft) < self.sqft_threshold:
			return False
		if int(built) < self.built_threshold:
			return False
		if int(price) < self.price_threshold:
			return False
		return True

	def add_house_datas(self, anchor_tags, zip_code):
		for a in anchor_tags:
			url = a['href']
			addr = a.text
			if self.debug:
				print('  Trying ' + url)
			try:
				self.driver.get(url)
				# if self.debug:
				# 	print('    connected, waiting for list')
			except TimeoutException:
				self.log_file.write(url + " took too long to open.")
				self.driver.execute_script("window.stop();")

			# Wait until the list of interest is generated by the javascript
			try:
				element = WebDriverWait(self.driver, 5).until(EC.visibility_of_element_located((By.XPATH, '//dl')))
			except TimeoutException:
				if self.debug:
					self.log_file.write(url + " failed (took too long).")
				continue
			# if self.debug:
			# 	print('    got list, finding dds')
			# Get all the values of the description list
			dds = element.find_elements_by_tag_name('dd')
			# if self.debug:
			# 	print('    found dds, finding price object')

			if (len(dds) < 4):
				if self.debug:
					# self.fails += 1
					# self.fail_urls.append(url)
					# print("  Retrieval from " + url + " failed (len of dl).")
					self.log_file.write(url + " failed (len of dl).")
				continue
			beds = dds[0].text
			baths = dds[1].text
			sqft = dds[2].text.replace(',', '')
			built = dds[-1].text
			price_obj = self.driver.find_element_by_css_selector('span.price-elements')
			# if self.debug:
			# 	print('    found price object, extracting price')
			price = self.driver.execute_script("""
								  		  return jQuery(arguments[0]).contents().filter(function() {
								    	  	  return this.nodeType == Node.TEXT_NODE;
								  		  }).text();
								  		  """, price_obj).strip().replace(',', '')
			# if self.debug:
			# 	print('    found price, checking validity')
			if not self.data_acceptable(beds, baths, sqft, built, price):
				if self.debug:
					# print("  Retrieval from " + url + " failed (bad data).")
					# print('    values: ' + beds + ' ' + baths + ' ' + sqft + ' ' + built + ' ' + price)
					self.log_file.write(url + " failed (bad data). " + beds + ' ' + baths + ' ' + sqft + ' ' + built + ' ' + price)
				continue
			### TODO: generate longitude and latitude
			# if self.debug:
			# 	print('    data valid, writing to file')
			self.writer.writerow((-1, -1, addr, self.city, self.state, zip_code, url, int(beds), int(baths), int(sqft), int(built), int(price)))
			if self.debug:
				self.datapoints += 1
				if self.datapoints % 100 == 0:
					print('  ' + str(self.datapoints) + ' addresses collected.')
		return

	def scrape_zip(self, zip_code):
		has_next_page = True
		page_count = 0
		total_addresses = 0
		while has_next_page:
			page_count += 1
			last_page = str(page_count)
			url = self.base_url + zip_code + '/p' + str(page_count)
			r = requests.get(url)
			if r.status_code != 200:
				if self.debug:
					print('Received status code ' + str(r.status_code) + ' from ' + url)
				continue
			html = r.content
			soup = BeautifulSoup(html, 'html.parser')
			# Find the list of addresses
			addresses_list = soup.find('ul', {'class' : 'lists-zebra'})
			page_links = addresses_list.find_all('a')
			self.add_house_datas(page_links, zip_code)
			if len(soup.find_all('a', {'rel' : 'next'})) == 0:
				has_next_page = False
		return

	"""
	Returns 
		with attributes (* means for debugging):
			longitute, latitute, street_addr*, city*, state*, zip, url*, bedrooms, bathrooms, sq_ft, built
		and label:
			price
	"""
	def scrape(self):
		r = requests.get(self.url)
		html = r.content
		soup = BeautifulSoup(html, 'html.parser')
		# Find the list of zip codes for this city in the HTML
		zip_code_list = soup.find('ul', {'class' : 'lists-zebra'})
		links = zip_code_list.find_all('a')
		zip_codes = [link.get_text() for link in links]
		total = len(links)
		if self.debug:
			completed = 0
		for zip_code in zip_codes:
			last_zip = zip_code
			self.scrape_zip(zip_code)
			if self.debug:
				completed += 1
				print('Completed Zip: ' + zip_code + ' (' + str(total-completed) + '/' + str(total) + ' remaining).')
		self.out_file.close()
		self.log_file.close()
		return





class CityScraper2:

	base_url = 'https://www.homes.com/home-prices/'

	def __init__(self, state, city, url, file_loc, addrs_per_zip=100, offset=0, zip_ignores=[], debug=False):
		self.state = state
		self.city = city
		self.url = url
		self.debug = debug
		self.addrs_per_zip = addrs_per_zip
		self.zip_ignores = set(zip_ignores)
		self.offset = offset
		self.out_file = open(file_loc, 'wt', newline='')
		self.writer = csv.writer(self.out_file)
		self.writer.writerow(('long', 'lat', 'addr', 'city', 'state', 'zip', 'url', 'beds', 'baths', 'sqft', 'date', 'price'))
		self.beds_threshold = 7
		self.baths_threshold = 7
		self.sqft_threshold = 300
		self.built_threshold = 1899
		self.price_threshold = 30000
		self.driver = webdriver.Firefox()
		self.driver.set_page_load_timeout(5)
		if debug:
			self.log_file = open('log.txt', 'w')
			self.fails = 0
			self.fail_urls = []
			self.datapoints = 0
			print('CityScraper finished init')

	# Returns whether the information is acceptable
	def data_acceptable(self, beds, baths, sqft, built, price):
		if not (len(beds) > 0 and len(baths) > 0 and len(sqft) > 0 and len(built) > 0 and len(price) > 0):
			return False
		if not (beds.isdigit() and baths.isdigit() and sqft.isdigit() and built.isdigit() and price.isdigit()):
			return False
		if int(beds) > self.beds_threshold or int(beds) < 1:
			return False
		if int(baths) > self.baths_threshold or int(baths) < 1:
			return False
		if int(sqft) < self.sqft_threshold:
			return False
		if int(built) < self.built_threshold:
			return False
		if int(price) < self.price_threshold:
			return False
		return True

	def scrape_page(self, a, zip_code):
		url = a['href']
		addr = a.text
		if self.debug:
			print('  Trying ' + url)
		try:
			self.driver.get(url)
		except TimeoutException:
			self.log_file.write(url + " took too long to open.")
			self.driver.execute_script("window.stop();")

		# Wait until the list of interest is generated by the javascript
		try:
			element = WebDriverWait(self.driver, 5).until(EC.visibility_of_element_located((By.XPATH, '//dl')))
		except TimeoutException:
			if self.debug:
				self.log_file.write(url + " failed (took too long).")
			return

		dds = element.find_elements_by_tag_name('dd')

		if (len(dds) < 4):
			if self.debug:
				self.log_file.write(url + " failed (len of dl).")
			return
		beds = dds[0].text
		baths = dds[1].text
		sqft = dds[2].text.replace(',', '')
		built = dds[-1].text
		price_obj = self.driver.find_element_by_css_selector('span.price-elements')
		price = self.driver.execute_script("""
							  		  return jQuery(arguments[0]).contents().filter(function() {
							    	  	  return this.nodeType == Node.TEXT_NODE;
							  		  }).text();
							  		  """, price_obj).strip().replace(',', '')
		if not self.data_acceptable(beds, baths, sqft, built, price):
			if self.debug:
				self.log_file.write(url + " failed (bad data). " + beds + ' ' + baths + ' ' + sqft + ' ' + built + ' ' + price)
			return

		self.writer.writerow((-1, -1, addr, self.city, self.state, zip_code, url, int(beds), int(baths), int(sqft), int(built), int(price)))
		if self.debug:
			self.datapoints += 1
			if self.datapoints % 10 == 0:
				print('  ' + str(self.datapoints) + ' addresses collected.')
		return

	def get_links_from_zip(self, zip_code):
		has_next_page = True
		seen = 0
		links = []
		page_count = 0
		while has_next_page and len(links) < self.addrs_per_zip:
			page_count += 1
			url = self.base_url + zip_code + '/p' + str(page_count)
			r = requests.get(url)
			if r.status_code != 200:
				if self.debug:
					print('Received status code ' + str(r.status_code) + ' from ' + url)
				continue
			html = r.content
			soup = BeautifulSoup(html, 'html.parser')
			if len(soup.find_all('a', {'rel' : 'next'})) == 0:
				has_next_page = False
			addresses_list = soup.find('ul', {'class' : 'lists-zebra'})
			page_links = addresses_list.find_all('a')
			start = 0
			if seen < self.offset:
				start = self.offset - seen
				if start >= len(page_links):
					seen += len(page_links)
					continue
				seen = self.offset
			links_on_page = len(page_links) - start
			if links_on_page + len(page_links) > self.addrs_per_zip:
				links += page_links[start:start+self.addrs_per_zip-total_addresses]
			else:
				links += page_links[start:]
		return [(link, zip_code) for link in links]

	"""
	Returns 
		with attributes (* means for debugging):
			longitute, latitute, street_addr*, city*, state*, zip, url*, bedrooms, bathrooms, sq_ft, built
		and label:
			price
	"""
	def scrape(self):
		r = requests.get(self.url)
		html = r.content
		soup = BeautifulSoup(html, 'html.parser')
		# Find the list of zip codes for this city in the HTML
		zip_code_list = soup.find('ul', {'class' : 'lists-zebra'})
		links = zip_code_list.find_all('a')
		zip_codes = [link.get_text() for link in links]
		total = len(links)
		links_to_scrape = []
		for zip_code in zip_codes:
			if zip_code in self.zip_ignores:
				continue
			last_zip = zip_code
			links_to_scrape += self.get_links_from_zip(zip_code)
			if self.debug:
				print('Got links from ' + zip_code + '. ' + str(len(links_to_scrape)) + ' links to scrape.')

		shuffle(links_to_scrape)
		if self.debug:
			print('BEGINNING SCRAPING')
		for anchor,zip_code in links_to_scrape:
			self.scrape_page(anchor, zip_code)
		self.out_file.close()
		self.log_file.close()
		return


# cs = CityScraper('PA', 'Pittsburgh', 'https://www.homes.com/home-prices/pittsburgh-pa/', '../data/pittsburgh_housing_data.csv', debug=True)
# cs.scrape()


cs2 = CityScraper2('PA', 'Pittsburgh', 'https://www.homes.com/home-prices/pittsburgh-pa/', '../data/pittsburgh_housing_data_2.csv',\
				   zip_ignores=['15201'], debug=True)
cs2.scrape()